{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Meeting Minutes Generation with Gradio Integration.\n",
        "\n",
        "# This script generates meeting minutes from an audio file using a quantized language model.\n",
        "\"\"\"\n",
        "\n",
        "# %% [markdown]\n",
        "# # Install dependencies\n",
        "# Uncomment and run the following in Colab or if dependencies are not installed:\n",
        "# ```shell\n",
        "# !pip install -q gradio requests torch bitsandbytes transformers sentencepiece accelerate openai\n",
        "# ```"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TextStreamer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        ")\n",
        "import gradio as gr\n",
        "from IPython.display import Markdown, display, update_display"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Constants\n",
        "AUDIO_MODEL = \"whisper-1\"  # Whisper model for transcription\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  # Quantized model for text generation\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Mount Google Drive if needed\n",
        "# Uncomment below if using Colab with audio files saved in Google Drive."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "\n",
        "audio_filename = \"/content/drive/MyDrive/llms/denver_extract.mp3\"  # Update as needed"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configure quantization with BitsAndBytesConfig\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "# Load tokenizer and quantized model\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLAMA, device_map=\"auto\", quantization_config=quant_config\n",
        ")\n",
        "model.to(\"cuda\")  # Ensure model runs on GPU if available\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define Gradio interface function\n",
        "def generate_minutes(audio):\n",
        "    # Transcribe audio using Whisper or local pipeline if Whisper unavailable\n",
        "    audio_file = open(audio, \"rb\")\n",
        "    transcription = openai.Audio.transcribe(\n",
        "        model=AUDIO_MODEL, file=audio_file, response_format=\"text\"\n",
        "    )\n",
        "\n",
        "    # Prepare prompt\n",
        "    system_message = (\n",
        "        \"Generate minutes with summary, discussion points, and actions in markdown.\"\n",
        "    )\n",
        "    user_prompt = f\"Transcript of a meeting:\\n{transcription}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    # Tokenize and generate response\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "    outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "\n",
        "    # Decode and return response\n",
        "    response = tokenizer.decode(outputs[0])\n",
        "    return response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup Gradio UI\n",
        "gr.Interface(\n",
        "    fn=generate_minutes,\n",
        "    inputs=\"audio\",\n",
        "    outputs=\"markdown\",\n",
        "    title=\"Meeting Minutes Generator\",\n",
        "    description=\"Upload an audio file to generate meeting minutes with a quantized model.\",\n",
        ").launch(share=True)\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Optional: Saving as Markdown\n",
        "# This output can be saved as Markdown or text if needed.\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}